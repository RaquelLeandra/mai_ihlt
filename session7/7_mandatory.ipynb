{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.metrics import jaccard_distance\n",
    "import csv\n",
    "from nltk import ne_chunk,pos_tag,word_tokenize\n",
    "from nltk.chunk import tree2conllstr\n",
    "from nltk.tree import Tree\n",
    "from copy import deepcopy\n",
    "from nltk.parse import CoreNLPParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trial_path = 'data/trial/STS.input.txt'\n",
    "trial_gs_path = 'data/trial/STS.gs.txt'\n",
    "trial_df = pd.read_csv(trial_path, sep='\\t', lineterminator='\\n', names=['sentence0','sentence1'], header=None, quoting=csv.QUOTE_NONE)\n",
    "trial_gs = pd.read_csv(trial_gs_path, sep='\\t', lineterminator='\\n', names=['labels'], header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data, ne_parser_function):\n",
    "    # To see the effect of the desambiguation we do a preprocess only with it. Generaly we will use a more complete \n",
    "    # preprocessing function. \n",
    "    data = data.fillna('')\n",
    "    first = lambda x: [a[0] for a in x]\n",
    "    for column in data.columns:\n",
    "        data[column] = data[column].apply(ne_parser_function)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_simmilarity(df):\n",
    "    guess = pd.DataFrame()\n",
    "    for i in df.index:\n",
    "        guess.loc[i,'labels'] = 1. - jaccard_distance(set(df.loc[i,'sentence0']), set(df.loc[i,'sentence1']))\n",
    "    return guess\n",
    "\n",
    "def analyzeResults(results):\n",
    "    guess_lex = lexical_simmilarity(results)\n",
    "    pearson    = pearsonr(trial_gs['labels'], guess_lex['labels'])[0]\n",
    "    print(guess_lex)\n",
    "    print(pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def na_parser(tree):\n",
    "    parsed_array = []\n",
    "    for chunk in tree: \n",
    "        if type(chunk) == Tree:\n",
    "            word = ' '.join(leaf[0] for leaf in chunk)\n",
    "            parsed_array.append(word.lower())\n",
    "        else: \n",
    "            parsed_array.append(chunk[0].lower())\n",
    "    \n",
    "    return [i for i in parsed_array if i.isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\n",
    "# java -mx4g -cp C:\\stanford-corenlp-full-2018-10-05\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n",
    "\n",
    "def nlp_parser(tagged):\n",
    "    parsed     = []\n",
    "    last_tag   = None\n",
    "    start_index = 0\n",
    "    for index, node in enumerate(tagged):\n",
    "        tag = node[1]\n",
    "        if (tag == 'O' or tag != last_tag) and (start_index != index):\n",
    "            token = ' '.join([pair[0].lower() for pair in tagged[start_index:index]])\n",
    "            if token.isalnum():\n",
    "                parsed.append(token)\n",
    "            last_tag = tag\n",
    "            start_index = index\n",
    "    return parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       labels\n",
      "id1  0.272727\n",
      "id2  0.250000\n",
      "id3  0.636364\n",
      "id4  0.400000\n",
      "id5  0.090909\n",
      "id6  0.107143\n",
      "-0.40498149435177305\n"
     ]
    }
   ],
   "source": [
    "results    = preprocessing(deepcopy(trial_df), lambda x : na_parser(ne_chunk(pos_tag(word_tokenize(x)), binary=True)))\n",
    "analyzeResults(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       labels\n",
      "id1  0.272727\n",
      "id2  0.181818\n",
      "id3  0.636364\n",
      "id4  0.400000\n",
      "id5  0.090909\n",
      "id6  0.107143\n",
      "-0.34327956222578704\n"
     ]
    }
   ],
   "source": [
    "results    = preprocessing(deepcopy(trial_df), lambda x : nlp_parser(parser.tag(word_tokenize(x))))\n",
    "analyzeResults(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
