{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "import urllib.request\n",
    "from nltk.metrics import jaccard_distance\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read all pairs of sentences of the trial set within the\n",
    "evaluation framework of the project.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id1 ['The bird is bathing in the sink.', 'Birdie is washing itself in the water basin.']\n",
      "id2 ['In May 2010, the troops attempted to invade Kabul.', 'The US army invaded Kabul on May 7th last year, 2010.']\n",
      "id3 ['John said he is considered a witness but not a suspect.', '\"He is not a suspect anymore.\" John said.']\n",
      "id4 ['They flew out of the nest in groups.', 'They flew into the nest together.']\n",
      "id5 ['The woman is playing the violin.', 'The young lady enjoys listening to the guitar.']\n",
      "id6 ['John went horse back riding at dawn with a whole group of friends.', 'Sunrise at dawn is a magnificent view to take in if you wake up early enough for it.']\n"
     ]
    }
   ],
   "source": [
    "pairs = {}\n",
    "\n",
    "with open('trial/STS.input.txt', 'r') as f:\n",
    "    for l in f:\n",
    "        sid = l.split('\\t')[0]\n",
    "        s1  = l.split('\\t')[1]\n",
    "        s2  = l.split('\\t')[2][:-1] # Remove the \\n character\n",
    "        pairs[sid] = [s1, s2]\n",
    "        print(sid, pairs[sid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute their similarities by considering words and Jaccard distance in lemmas.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: id1 similarity [0-1]: 0.33333333333333337\n",
      "id: id2 similarity [0-1]: 0.4117647058823529\n",
      "id: id3 similarity [0-1]: 0.5714285714285714\n",
      "id: id4 similarity [0-1]: 0.4545454545454546\n",
      "id: id5 similarity [0-1]: 0.16666666666666663\n",
      "id: id6 similarity [0-1]: 0.13793103448275867\n"
     ]
    }
   ],
   "source": [
    "simmilarities = []\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    pairs = pos_tag(words)\n",
    "    lemms = []\n",
    "    for pair in pairs:\n",
    "        if pair[1][0] in {'N', 'V'}:\n",
    "            lemms.append(wnl.lemmatize(pair[0].lower(), pos=pair[1][0].lower()))\n",
    "        else:\n",
    "            lemms.append(pair[0].lower())\n",
    "    return lemms\n",
    "\n",
    "for sid in pairs:\n",
    "    s1      = pairs[sid][0]\n",
    "    s2      = pairs[sid][1]\n",
    "    lemms_1 = set(lemmatize(s1))\n",
    "    lemms_2 = set(lemmatize(s2))\n",
    "    jaccard_simmilarity = 1 - jaccard_distance(lemms_1, lemms_2)\n",
    "    simmilarities.append(jaccard_simmilarity) \n",
    "    print('id:', sid, 'similarity [0-1]:', jaccard_simmilarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the previous results with gold standard by giving the pearson correlation between them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5790860088205633\n"
     ]
    }
   ],
   "source": [
    "gs = {}\n",
    "\n",
    "with open('trial/STS.gs.txt', 'r') as f:\n",
    "    for l in f:\n",
    "        sid     = l.split('\\t')[0]\n",
    "        value   = abs( int(l.split('\\t')[1])-5)    \n",
    "        gs[sid] = value\n",
    "\n",
    "refs = list(gs.values())\n",
    "print(pearsonr(refs, simmilarities)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a correlation coefficient of 0.5790860088205633. \n",
    "**This correlation coefficient is better than the one in the previous exersice. This meaning that working with lemmas is better for this example.** But in this case it only improved the value of the similarity for one of the pairs that was indeed similar (id2).\n",
    "\n",
    "Working with lemmas should be generally better because the same words implying the same meaning may appear in a pair of sentences, but if they have any morphological variation they will be trated as separated words in the Jaccard Distance calculation.\n",
    "\n",
    "_Also, as in the previus mandatory exercise:_\n",
    "\n",
    "> As we are comparing two arrays of similarity values, we obtain a positive correlation. \n",
    "\n",
    "> This value is a little bigger than 0.5, this means that there is little correlation between the two arrays, so probably the Jaccard distance isn't the best way to measure the semantic similarity between sentences. \n",
    "\n",
    "> These results are due to the definition of Jaccard distance. This definition is fully based on set theory and does not take into account the semantic relationship between words (like synonymity)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
