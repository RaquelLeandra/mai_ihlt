{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from nltk.wsd import lesk\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.stats import pearsonr\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.tokenize import WhitespaceTokenizer\n",
    "#from nltk.metrics import jaccard_distance\n",
    "#from scipy.stats import pearsonr\n",
    "#from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 2) (6, 1)\n"
     ]
    }
   ],
   "source": [
    "trial_path    = 'trial/STS.input.txt'\n",
    "trial_gs_path = 'trial/STS.gs.txt'\n",
    "trial_df      = pd.read_csv(trial_path,    sep='\\t', lineterminator='\\n', names=['sentence0','sentence1'], header=None, quoting=csv.QUOTE_NONE)\n",
    "trial_gs      = pd.read_csv(trial_gs_path, sep='\\t', lineterminator='\\n', names=['labels'],                header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "print(trial_df.shape, trial_gs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity [0-1]: 0.33333333333333337\n",
      "similarity [0-1]: 0.33333333333333337\n",
      "similarity [0-1]: 0.5714285714285714\n",
      "similarity [0-1]: 0.33333333333333337\n",
      "similarity [0-1]: 0.16666666666666663\n",
      "similarity [0-1]: 0.09999999999999998\n"
     ]
    }
   ],
   "source": [
    "length = trial_df.shape[0]\n",
    "sims = []\n",
    "\n",
    "for i in range(length):\n",
    "    semantics = []\n",
    "\n",
    "    for column in trial_df:\n",
    "        senten   = trial_df[column][i]\n",
    "        tokens   = word_tokenize(senten)\n",
    "        tagged   = pos_tag(tokens)\n",
    "        semantic = set()\n",
    "\n",
    "        for idx, token in enumerate(tokens):\n",
    "            context = [i for i in tokens if i != token]\n",
    "            tag     = tagged[idx][1][0].lower()\n",
    "            synset  = lesk(context, token.lower(), tag)\n",
    "\n",
    "            #print(token, synset, tag)\n",
    "            if synset:\n",
    "                semantic.add(synset) \n",
    "            else:\n",
    "                semantic.add(token.lower())\n",
    "\n",
    "        #print(senten)\n",
    "        #print(semantic)\n",
    "        semantics.append(semantic)\n",
    "        \n",
    "    sims.append(1 - jaccard_distance(semantics[0], semantics[1]))\n",
    "    print('similarity [0-1]:', sims[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6206712050540985\n",
      "[5, 4, 3, 2, 1, 0]\n",
      "[0.33333333333333337, 0.33333333333333337, 0.5714285714285714, 0.33333333333333337, 0.16666666666666663, 0.09999999999999998]\n"
     ]
    }
   ],
   "source": [
    "gs = [5, 4, 3, 2, 1, 0]\n",
    "\n",
    "print(pearsonr(gs, sims)[0])\n",
    "print(gs)\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphy_tag(nltk_tag):\n",
    "\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return wn.NOUN\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    w_tokenizer = WhitespaceTokenizer()\n",
    "    tokenized_text = w_tokenizer.tokenize(text)\n",
    "    list_tags = pos_tag(tokenized_text)\n",
    "    tags = {w:morphy_tag(tag) for w,tag in list_tags}\n",
    "    return [lemmatizer.lemmatize(w, tags[w]) for w in tokenized_text]\n",
    "\n",
    "def apply_lesk_to_text(text):    \n",
    "    return [lesk(text, word) for word in text]\n",
    "\n",
    "def preprocessing(data):\n",
    "    # todo: better handling of na\n",
    "    data = data.fillna('')\n",
    "    for column in data.columns:\n",
    "        print(column)\n",
    "        # remove the digits and puntuation\n",
    "        data[column] = data[column].str.replace('\\d+', '')\n",
    "        # convert to lowercase\n",
    "        data[column] = data[column].str.replace('\\W+', ' ')\n",
    "        # replace continuous white spaces by a single one\n",
    "        data[column] = data[column].str.replace('\\s+', ' ')\n",
    "        # words to lower\n",
    "        data[column] = data[column].str.lower()\n",
    "        # lematize\n",
    "        data[column] = data[column].apply(lemmatize_text)\n",
    "        # data[column] = data[column].str.split()\n",
    "        # desambiguate \n",
    "        data[column] = data[column].apply(apply_lesk_to_text)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'birdie is washing itself on the sink basine'\n",
    "lemmatize_text(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trial_df = preprocessing(trial_df)\n",
    "trial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_df.loc['id1','sentence0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('was')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_df.loc['id1','sentence1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesk(trial_df.loc['id1','sentence0'], 'bird')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lexical_simmilarity(df):\n",
    "    guess = pd.DataFrame()\n",
    "    for i in df.index:\n",
    "        guess.loc[i,'labels'] = 1 - jaccard_distance(set(df.loc[i,'sentence0']), set(df.loc[i,'sentence1']))\n",
    "    return guess\n",
    "\n",
    "\n",
    "guess_lex = lexical_simmilarity(trial_df)\n",
    "guess_lex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pearsonr(trial_gs['labels'], guess_lex['labels'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trial_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_df.to_csv('potato.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
