{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Textual Similarity in SemEval 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import nltk, string\n",
    "from nltk import pos_tag\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "from sklearn.metrics.pairwise import manhattan_distances as md\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
    "from sklearn.metrics import jaccard_similarity_score as jsc\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "# We will use the same scaler over all the notebook\n",
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and concatenate the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/test-gold/STS.input.MSRpar.txt\n",
      "../data/test-gold/STS.input.MSRvid.txt\n",
      "../data/test-gold/STS.input.SMTeuroparl.txt\n",
      "../data/test-gold/STS.input.surprise.SMTnews.txt\n",
      "../data/test-gold/STS.input.surprise.OnWN.txt\n",
      "../data/train/STS.input.MSRpar.txt\n",
      "../data/train/STS.input.MSRvid.txt\n",
      "../data/train/STS.input.SMTeuroparl.txt\n",
      "../data/test-gold/STS.input.MSRpar.txt\n",
      "../data/test-gold/STS.input.MSRvid.txt\n",
      "../data/test-gold/STS.input.SMTeuroparl.txt\n",
      "../data/test-gold/STS.input.surprise.SMTnews.txt\n",
      "../data/test-gold/STS.input.surprise.OnWN.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2234, 2), (2234, 1), (3108, 2), (3108, 1))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = '../data/train/'\n",
    "test_path = '../data/test-gold/'\n",
    "\n",
    "def load_and_concat(data_path):\n",
    "    files = os.listdir(data_path)\n",
    "    all_data = pd.DataFrame(columns=['sentence0','sentence1'])\n",
    "    all_labels = pd.DataFrame(columns=['labels'])\n",
    "    for file in files: \n",
    "        path = data_path + file\n",
    "        if 'input' in file:\n",
    "            print(path)\n",
    "            fd = pd.read_csv(path, sep='\\t', lineterminator='\\n', names=['sentence0','sentence1'], header=None, quoting=csv.QUOTE_NONE)\n",
    "            all_data = all_data.append(fd)\n",
    "            fd = pd.read_csv(path.replace('input','gs'), sep='\\t', lineterminator='\\n', names=['labels'], header=None, quoting=csv.QUOTE_NONE)\n",
    "            all_labels = all_labels.append(fd,ignore_index=True)\n",
    "    return all_data.reset_index(drop=True), all_labels.reset_index(drop=True)\n",
    "\n",
    "# We save appart the test befor preprocessing it to look at the badly classified sentences later\n",
    "original_test, test_gs = load_and_concat(test_path)\n",
    "\n",
    "\n",
    "train_df, train_gs = load_and_concat(train_path)\n",
    "test_df, test_gs = load_and_concat(test_path)\n",
    "\n",
    "train_df.shape, train_gs.shape,test_df.shape, test_gs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply a spell checker to train and test\n",
    "As this process is very slow we saved the datasets after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_spell(text):\n",
    "    \"\"\"\n",
    "    This functions apply the spell checker to a sentence, ignoring the propper nouns. \n",
    "    (Which are detected using a pretrained Perceptron pos tagger)\n",
    "    \"\"\"\n",
    "    spell = SpellChecker()\n",
    "    misspelled = spell.unknown(text.split())\n",
    "    tagger = PerceptronTagger()\n",
    "    tagged_text = tagger.tag(text)\n",
    "    corrected_text = ''\n",
    "    for i, word in enumerate(text.split()):\n",
    "        if word in misspelled:\n",
    "            tag = tagged_text[i][1]\n",
    "            if tag != 'NNP':\n",
    "                word = spell.correction(word)\n",
    "        corrected_text +=word +' '\n",
    "    return corrected_text.strip()\n",
    "\n",
    "\n",
    "\n",
    "def correct_dataset(dataset):\n",
    "    for column in dataset.columns:\n",
    "        dataset[column] = dataset[column].apply(auto_spell)\n",
    "    return dataset\n",
    "\n",
    "# corrected_train = correct_dataset(train_df)\n",
    "# corrected_train.to_csv('corrected_train.csv')\n",
    "# corrected_test = correct_dataset(test_df)\n",
    "# corrected_test.to_csv('corrected_test.csv')\n",
    "\n",
    "train_df = pd.read_csv('corrected_train.csv', index_col=0)\n",
    "test_df = pd.read_csv('corrected_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before the preprocessing we extract some features \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    "    return 'n'\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatizes the text using the perceptron Pos Tagger\"\"\"\n",
    "    tagger = PerceptronTagger()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    w_tokenizer = WhitespaceTokenizer()\n",
    "    s_tokenized = w_tokenizer.tokenize(text)\n",
    "    s_tagged = tagger.tag(s_tokenized)\n",
    "    return [lemmatizer.lemmatize(w[0], penn_to_wn(w[1])) for w in s_tagged]\n",
    "\n",
    "\n",
    "def tagged_to_synset(word, tag):\n",
    "    \"\"\"Returns the synset of the word depending on its pos tag\"\"\"\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def sentence_lenght(s):\n",
    "    return len(s.split())\n",
    "\n",
    "\n",
    "def count_symbols(s):\n",
    "    count = lambda l1, l2: sum([1 for x in l1 if x in l2])\n",
    "    return count(s, set(string.punctuation))\n",
    "\n",
    "\n",
    "def count_nouns(s0):\n",
    "    tagger = PerceptronTagger()\n",
    "    s0_tags = tagger.tag(s0.split())\n",
    "    NN_s0 = [values[0] for values in s0_tags if values[1] == 'NN']\n",
    "    return len(NN_s0)\n",
    "\n",
    "\n",
    "def count_verbs(s0):\n",
    "    tagger = PerceptronTagger()\n",
    "    s0_tags = tagger.tag(s0.split())\n",
    "    V_s0 = [values[0] for values in s0_tags if values[1] == 'VBP']\n",
    "    return len(V_s0)\n",
    "\n",
    "\n",
    "def count_digits(s):\n",
    "    numbers = sum(c.isdigit() for c in s)\n",
    "    return numbers\n",
    "\n",
    "def remove_stop_words(s0):\n",
    "    new_vector = ' '.join(word for word in s0.split() if word.lower() not in list(stopwords.words('english')))\n",
    "    return new_vector\n",
    "\n",
    "def _get_word_synonyms(word):\n",
    "    word_synonyms = []\n",
    "    for synset in wn.synsets(word):\n",
    "        for lemma in synset.lemma_names():\n",
    "            word_synonyms.append(lemma)\n",
    "    return word_synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comon_stop_word_proportion(s0, s1):\n",
    "    stopwords_s0 = [word.lower() for word in s0.split() if word.lower() in list(stopwords.words('english'))]\n",
    "    stopwords_s1 = [word.lower() for word in s1.split() if word.lower() in list(stopwords.words('english'))]\n",
    "    common = len(set(stopwords_s0) & set(stopwords_s1))\n",
    "    if min(len(stopwords_s0), len(stopwords_s1)) > 0:\n",
    "        return common / min(len(stopwords_s0), len(stopwords_s1))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def calculate_jaccard(s0, s1):\n",
    "    lemms_0 = [a for a in lemmatize_text(s0) if a]\n",
    "    lemms_1 = [a for a in lemmatize_text(s1) if a]\n",
    "    jaccard_simmilarity = (1 - jaccard_distance(set(lemms_0), set(lemms_1)))\n",
    "    return jaccard_simmilarity\n",
    "\n",
    "def count_shared_words(s0, s1):\n",
    "    list3 = list(set(lemmatize_text(s0.lower())) & set(lemmatize_text(s1.lower())))\n",
    "    return len(list3)\n",
    "\n",
    "\n",
    "def count_common_propper_nouns(s0, s1):\n",
    "    tagger = PerceptronTagger()\n",
    "    s0_tags = tagger.tag(s0.split())\n",
    "    s1_tags = tagger.tag(s1.split())\n",
    "    NNP_s0 = [values[0] for values in s0_tags if values[1] == 'NNP']\n",
    "    NNP_s1 = [values[0] for values in s1_tags if values[1] == 'NNP']\n",
    "    return len(set(NNP_s0) & set(NNP_s1))\n",
    "\n",
    "\n",
    "def synonim_proportion(s0, s1):\n",
    "    syn_count = 0\n",
    "    for a in s0.split():\n",
    "        for b in s1.split():\n",
    "            if a == b:\n",
    "                are_syns = 1\n",
    "            else:\n",
    "                are_syns = len(set(_get_word_synonyms(a)) & set(_get_word_synonyms(b))) > 0\n",
    "            syn_count += are_syns\n",
    "    max_len = min([len(s0.split()), len(s1.split())])\n",
    "    return syn_count / max_len\n",
    "\n",
    "\n",
    "def sentence_similarity(sentence1, sentence2, similarity):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(lemmatize_text(sentence1))\n",
    "    sentence2 = pos_tag(lemmatize_text(sentence2))\n",
    "\n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    "\n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    "\n",
    "    score, count = 0.0, 0\n",
    "\n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        similarities = [similarity(synset, ss) for ss in synsets2 if similarity(synset, ss)]\n",
    "        try:\n",
    "            best_score = max(similarities)\n",
    "        except:\n",
    "            best_score = 0\n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score += best_score\n",
    "            count += 1\n",
    "    # Average the values\n",
    "    try:\n",
    "        score /= count\n",
    "    except:\n",
    "        score = 0\n",
    "    return score\n",
    "\n",
    "\n",
    "def sentence_similarity_information_content(sentence1, sentence2, similarity):\n",
    "    ''' compute the sentence similairty using information content from wordnet '''\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(lemmatize_text(sentence1))\n",
    "    sentence2 = pos_tag(lemmatize_text(sentence2))\n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    "    score, count = 0.0, 0\n",
    "    ppdb_score, align_cnt = 0, 0\n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        L = []\n",
    "        for ss in synsets2:\n",
    "            try:\n",
    "                L.append(similarity(synset, ss, brown_ic))\n",
    "            except:\n",
    "                continue\n",
    "        if L:\n",
    "            best_score = max(L)\n",
    "            score += best_score\n",
    "            count += 1\n",
    "    # Average the values\n",
    "    if count > 0: score /= count\n",
    "    return score\n",
    "\n",
    "def feature_extractor(dataset):\n",
    "    features = pd.DataFrame(columns=['sentence_0_lengh', 'sentence_1_lengh',\n",
    "                                     'number_of_nouns_s0', 'number_of_nouns_s1',\n",
    "                                     'number_of_verbs_s0', 'number_of_verbs_s1',\n",
    "                                     'number_of_symbols_s0', 'number_of_symbols_s1',\n",
    "                                     'number_of_digits_s0', 'number_of_digits_1',\n",
    "                                     'synonim_proportion', 'quantity_of_shared_words',\n",
    "                                     'proper_nouns_shared', 'jaccard_distance', 'path_similarity',\n",
    "                                     'wup_similarity', 'comon_stop_word_proportion', 'resnik_similarity',\n",
    "                                     'jcn_similarity','lin_similarity'])\n",
    "    # We count the stop word proportion before remove the stopwords\n",
    "    for index, row in dataset.iterrows():\n",
    "        s0 = row['sentence0']\n",
    "        s1 = row['sentence1']\n",
    "        features.loc[index, 'comon_stop_word_proportion'] = comon_stop_word_proportion(s0, s1)\n",
    "    \n",
    "    # We remove the stopwords to extract better features\n",
    "    for column in dataset.columns:\n",
    "        dataset[column] = dataset[column].apply(remove_stop_words)\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        s0 = row['sentence0']\n",
    "        s1 = row['sentence1']\n",
    "        features.loc[index, 'jaccard_distance'] = calculate_jaccard(s0, s1)\n",
    "        features.loc[index, 'resnik_similarity'] = sentence_similarity_information_content(s0, s1, wn.res_similarity)\n",
    "        features.loc[index, 'jcn_similarity'] = sentence_similarity_information_content(s0, s1, wn.jcn_similarity)\n",
    "        features.loc[index, 'lin_similarity'] = sentence_similarity_information_content(s0, s1, wn.lin_similarity)\n",
    "        features.loc[index, 'path_similarity'] = sentence_similarity(s0, s1, wn.path_similarity)\n",
    "        features.loc[index, 'wup_similarity'] = sentence_similarity(s0, s1, wn.wup_similarity)\n",
    "        features.loc[index, 'proper_nouns_shared'] = count_common_propper_nouns(s0, s1)\n",
    "        features.loc[index, 'quantity_of_shared_words'] = count_shared_words(s0, s1)\n",
    "        features.loc[index, 'synonim_proportion'] = synonim_proportion(s0, s1)\n",
    "        features.loc[index, 'sentence_0_lengh'] = sentence_lenght(s0)\n",
    "        features.loc[index, 'sentence_1_lengh'] = sentence_lenght(s1)\n",
    "        features.loc[index, 'number_of_nouns_s0'] = count_nouns(s0)\n",
    "        features.loc[index, 'number_of_nouns_s1'] = count_nouns(s1)\n",
    "        features.loc[index, 'number_of_verbs_s0'] = count_verbs(s0)\n",
    "        features.loc[index, 'number_of_verbs_s1'] = count_verbs(s1)\n",
    "        features.loc[index, 'number_of_symbols_s0'] = count_symbols(s0)\n",
    "        features.loc[index, 'number_of_symbols_s1'] = count_symbols(s1)\n",
    "        features.loc[index, 'number_of_digits_s0'] = count_digits(s0)\n",
    "        features.loc[index, 'number_of_digits_1'] = count_digits(s1)\n",
    "    # We scalate resnik similarity to avoid overflow problems on the classifiers\n",
    "    features['jcn_similarity'] = scaler.fit_transform(features[['jcn_similarity']].values)\n",
    "    features['resnik_similarity'] = scaler.fit_transform(features[['resnik_similarity']].values)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step may take a while\n",
    "train_features = feature_extractor(train_df)\n",
    "test_features = feature_extractor(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We scalate the features\n",
    "train_features_std = scaler.fit_transform(train_features)\n",
    "test_features_std = scaler.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence0\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(data, return_array=False):\n",
    "    data = data.fillna('')\n",
    "    for column in data.columns:\n",
    "        print(column)\n",
    "        # remove the digits and puntuation\n",
    "        data[column] = data[column].str.replace('\\d+', '')\n",
    "        # remove stopwords\n",
    "        data[column] = data[column].apply(remove_stop_words)\n",
    "        # convert to lowercase\n",
    "        data[column] = data[column].str.replace('\\W+', ' ')\n",
    "        # replace continuous white spaces by a single one\n",
    "        data[column] = data[column].str.replace('\\s+', ' ')\n",
    "        # words to lower\n",
    "        data[column] = data[column].str.lower()\n",
    "        # lematize\n",
    "        data[column] = data[column].apply(lemmatize_text)\n",
    "        if not return_array:\n",
    "            data[column] = data[column].str.join(' ')\n",
    "    return data\n",
    "\n",
    "train_df = preprocessing(train_df,return_array=False)\n",
    "test_df = preprocessing(test_df,return_array=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence0</th>\n",
       "      <th>sentence1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>source close sale say vivendi keep door open b...</td>\n",
       "      <td>source close sale say vivendi keep door open b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>micron declare first quarterly profit three year</td>\n",
       "      <td>micron s number also mark first quarterly prof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fine part fail republican effort force entice ...</td>\n",
       "      <td>perry say back senate s effort include fine fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>american anglican council represent episcopali...</td>\n",
       "      <td>american anglican council represent episcopali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tech load nasdaq composite rise 20 96 point 15...</td>\n",
       "      <td>technology lace nasdaq composite index ixic cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence0  \\\n",
       "0  source close sale say vivendi keep door open b...   \n",
       "1   micron declare first quarterly profit three year   \n",
       "2  fine part fail republican effort force entice ...   \n",
       "3  american anglican council represent episcopali...   \n",
       "4  tech load nasdaq composite rise 20 96 point 15...   \n",
       "\n",
       "                                           sentence1  \n",
       "0  source close sale say vivendi keep door open b...  \n",
       "1  micron s number also mark first quarterly prof...  \n",
       "2  perry say back senate s effort include fine fo...  \n",
       "3  american anglican council represent episcopali...  \n",
       "4  technology lace nasdaq composite index ixic cl...  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence0</th>\n",
       "      <th>sentence1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>problem likely mean corrective change shuttle ...</td>\n",
       "      <td>say problem need correct space shuttle fleet c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>technology lace nasdaq composite index ixic in...</td>\n",
       "      <td>broad standard door 500 index spx inch 3 point...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>let s huge black eye say publisher arthur chs ...</td>\n",
       "      <td>let s huge black eye arthur sulzberger newspap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sec chairman william donaldson say building co...</td>\n",
       "      <td>think three s build confidence cop beat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vivendi share close 1 9 percent 15 80 euro par...</td>\n",
       "      <td>new work vivendi share 1 4 percent 51829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence0  \\\n",
       "0  problem likely mean corrective change shuttle ...   \n",
       "1  technology lace nasdaq composite index ixic in...   \n",
       "2  let s huge black eye say publisher arthur chs ...   \n",
       "3  sec chairman william donaldson say building co...   \n",
       "4  vivendi share close 1 9 percent 15 80 euro par...   \n",
       "\n",
       "                                           sentence1  \n",
       "0  say problem need correct space shuttle fleet c...  \n",
       "1  broad standard door 500 index spx inch 3 point...  \n",
       "2  let s huge black eye arthur sulzberger newspap...  \n",
       "3            think three s build confidence cop beat  \n",
       "4           new work vivendi share 1 4 percent 51829  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore some lexical dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pearson:  0.7424601681998196\n",
      "test pearson:  0.5976646227151003\n"
     ]
    }
   ],
   "source": [
    "def lexical_simmilarity(df):\n",
    "    guess = pd.DataFrame()\n",
    "    for i in df.index:\n",
    "        guess.loc[i, 'labels'] = 1 - jaccard_distance(set(df.loc[i, 'sentence0']), set(df.loc[i, 'sentence1']))\n",
    "    return guess\n",
    "\n",
    "\n",
    "guess_lex_train = lexical_simmilarity(train_df)\n",
    "guess_lex_test = lexical_simmilarity(test_df)\n",
    "\n",
    "print('train pearson: ', pearsonr(guess_lex_train['labels'], train_gs['labels'])[0])\n",
    "print('test pearson: ', pearsonr(guess_lex_test['labels'], test_gs['labels'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_sims(dataset,symilarity_measure):\n",
    "    results = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        s0 = row['sentence0']\n",
    "        s1 = row['sentence1']\n",
    "        results.append(sentence_similarity(s0,s1, symilarity_measure))\n",
    "    return results\n",
    "\n",
    "def calculate_all_sims_ic(dataset,symilarity_measure):\n",
    "    results = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        s0 = row['sentence0']\n",
    "        s1 = row['sentence1']\n",
    "        results.append(sentence_similarity_information_content(s0,s1, symilarity_measure))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pearson:  0.5633017937153743\n",
      "test pearson:  0.5284083437429895\n"
     ]
    }
   ],
   "source": [
    "guess_lex_train = calculate_all_sims(train_df,wn.path_similarity)\n",
    "guess_lex_test = calculate_all_sims(test_df,wn.path_similarity)\n",
    "\n",
    "print('train pearson: ', pearsonr(guess_lex_train, train_gs['labels'])[0])\n",
    "print('test pearson: ', pearsonr(guess_lex_test, test_gs['labels'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wu Palmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pearson:  0.43967804489375517\n",
      "test pearson:  0.40097435468823356\n"
     ]
    }
   ],
   "source": [
    "guess_lex_train = calculate_all_sims(train_df,wn.wup_similarity)\n",
    "guess_lex_test = calculate_all_sims(test_df,wn.wup_similarity)\n",
    "\n",
    "print('train pearson: ', pearsonr(guess_lex_train, train_gs['labels'])[0])\n",
    "print('test pearson: ', pearsonr(guess_lex_test, test_gs['labels'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pearson:  0.44921340347276706\n",
      "test pearson:  0.39752191638094025\n"
     ]
    }
   ],
   "source": [
    "guess_lex_train = calculate_all_sims_ic(train_df,wn.lin_similarity)\n",
    "guess_lex_test = calculate_all_sims_ic(test_df,wn.lin_similarity)\n",
    "\n",
    "print('train pearson: ', pearsonr(guess_lex_train, train_gs['labels'])[0])\n",
    "print('test pearson: ', pearsonr(guess_lex_test, test_gs['labels'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnik Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pearson:  0.0\n",
      "test pearson:  0.0\n"
     ]
    }
   ],
   "source": [
    "guess_lex_train = calculate_all_sims_ic(train_df,wn.res_similarity)\n",
    "guess_lex_test = calculate_all_sims_ic(test_df,wn.res_similarity)\n",
    "\n",
    "print('train pearson: ', pearsonr(guess_lex_train, train_gs['labels'])[0])\n",
    "print('test pearson: ', pearsonr(guess_lex_test, test_gs['labels'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Explore the syntactic dimension alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the combination of both previous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preprocessing and in the feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence0\n",
      "sentence1\n",
      "No of words in the dictionary = 1783\n",
      "sentence0\n",
      "sentence1\n",
      "(2234, 1783)\n",
      "(3108, 1783)\n"
     ]
    }
   ],
   "source": [
    "def train_dictionary(df):\n",
    "    \n",
    "    sentences_tokenized = df.sentence0.tolist() + df.sentence1.tolist()\n",
    "    \n",
    "    dictionary = corpora.Dictionary(sentences_tokenized)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.8)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    return dictionary\n",
    "    \n",
    "def get_vectors(df, dictionary):\n",
    "    \n",
    "    sentence0_vec = [dictionary.doc2bow(text) for text in df.sentence0.tolist()]\n",
    "    sentence1_vec = [dictionary.doc2bow(text) for text in df.sentence1.tolist()]\n",
    "    \n",
    "    sentence0_csc = gensim.matutils.corpus2csc(sentence0_vec, num_terms=len(dictionary.token2id))\n",
    "    sentence1_csc = gensim.matutils.corpus2csc(sentence1_vec, num_terms=len(dictionary.token2id))\n",
    "    \n",
    "    return sentence0_csc.transpose(),sentence1_csc.transpose()\n",
    "\n",
    "tokenized_train = preprocessing(train_df, return_array = True)\n",
    "dictionary = train_dictionary(tokenized_train)\n",
    "print (\"No of words in the dictionary = %s\" %len(dictionary))\n",
    "\n",
    "tokenized_test = preprocessing(test_df, return_array = True)\n",
    "\n",
    "q1_csc, q2_csc = get_vectors(tokenized_train, dictionary)\n",
    "q1_csc_test, q2_csc_test = get_vectors(tokenized_test, dictionary)\n",
    "\n",
    "print (q1_csc.shape)\n",
    "print (q1_csc_test.shape)\n",
    "\n",
    "train_bog = np.concatenate((q1_csc.todense(), q2_csc.todense()), axis=1)\n",
    "test_bog = np.concatenate((q1_csc_test.todense(), q2_csc_test.todense()), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the feature information to the bag of words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bog_extended = pd.concat([pd.DataFrame(train_bog),train_features],axis=1)\n",
    "test_bog_extended = pd.concat([pd.DataFrame(test_bog),test_features],axis=1)\n",
    "\n",
    "train_bog_extended_std = pd.concat([pd.DataFrame(train_bog),pd.DataFrame(train_features_std,columns=train_features.columns)],axis=1)\n",
    "test_bog_extended_std = pd.concat([pd.DataFrame(test_bog),pd.DataFrame(test_features_std,columns=test_features.columns)],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test pearson:  0.5444314437398093\n"
     ]
    }
   ],
   "source": [
    "model_nn = MLPRegressor(hidden_layer_sizes=(100,100),validation_fraction=0.3, alpha=0.3,warm_start=False,max_iter=1000)\n",
    "model_nn.fit(train_bog_extended_std,train_gs['labels'])\n",
    "\n",
    "test_predicted = model_nn.predict(test_bog_extended_std)\n",
    "print('test pearson: ', pearsonr(test_predicted, test_gs['labels'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test pearson:  0.7178333373880817\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(n_jobs=-1,n_estimators=1000)\n",
    "rfr.fit(train_bog_extended,train_gs['labels'])\n",
    "\n",
    "test_predicted = rfr.predict(test_bog_extended)\n",
    "print('test pearson: ', pearsonr(test_predicted, test_gs['labels'])[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
