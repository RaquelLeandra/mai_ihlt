{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import nltk, string\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "from sklearn.metrics.pairwise import manhattan_distances as md\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
    "from sklearn.metrics import jaccard_similarity_score as jsc\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from nltk.metrics import jaccard_distance\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.metrics import jaccard_distance \n",
    "from spellchecker import SpellChecker\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.metrics import jaccard_distance\n",
    "\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement\n",
    "- Use data set and description of task Semantic Textual Similarity in SemEval 2012.\n",
    "- Implement some approaches to detect paraphrase using sentence similarity metrics.\n",
    "    + Explore some lexical dimensions. (Only word)\n",
    "    + Explore the syntactic dimension alone. (Word respect to sentence)\n",
    "    + Explore the combination of both previous.\n",
    "- Add new components at your choice (optional).\n",
    "- Compare and comment the results achieved by these approaches among them and among the official results.\n",
    "- Send files to raco in IHLT STS Project before the oral presentation:\n",
    "    + Jupyter notebook: sts-[Student1]-[Student2].ipynb\n",
    "    + Slides: sts-[Student1]-[Student2].pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/test-gold/STS.input.MSRpar.txt\n",
      "../data/test-gold/STS.input.MSRvid.txt\n",
      "../data/test-gold/STS.input.SMTeuroparl.txt\n",
      "../data/test-gold/STS.input.surprise.SMTnews.txt\n",
      "../data/test-gold/STS.input.surprise.OnWN.txt\n",
      "../data/train/STS.input.MSRpar.txt\n",
      "../data/train/STS.input.MSRvid.txt\n",
      "../data/train/STS.input.SMTeuroparl.txt\n",
      "../data/test-gold/STS.input.MSRpar.txt\n",
      "../data/test-gold/STS.input.MSRvid.txt\n",
      "../data/test-gold/STS.input.SMTeuroparl.txt\n",
      "../data/test-gold/STS.input.surprise.SMTnews.txt\n",
      "../data/test-gold/STS.input.surprise.OnWN.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2234, 2), (2234, 1), (3108, 2), (3108, 1))"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = '../data/train/'\n",
    "test_path = '../data/test-gold/'\n",
    "\n",
    "def load_and_concat(data_path):\n",
    "    files = os.listdir(data_path)\n",
    "    all_data = pd.DataFrame(columns=['sentence0','sentence1'])\n",
    "    all_labels = pd.DataFrame(columns=['labels'])\n",
    "    for file in files: \n",
    "        path = data_path + file\n",
    "        if 'input' in file:\n",
    "            print(path)\n",
    "            fd = pd.read_csv(path, sep='\\t', lineterminator='\\n', names=['sentence0','sentence1'], header=None, quoting=csv.QUOTE_NONE)\n",
    "            all_data = all_data.append(fd)\n",
    "            fd = pd.read_csv(path.replace('input','gs'), sep='\\t', lineterminator='\\n', names=['labels'], header=None, quoting=csv.QUOTE_NONE)\n",
    "            all_labels = all_labels.append(fd,ignore_index=True)\n",
    "    return all_data.reset_index(drop=True), all_labels.reset_index(drop=True)\n",
    "\n",
    "original_test, test_gs = load_and_concat(test_path)\n",
    "\n",
    "\n",
    "train_df, train_gs = load_and_concat(train_path)\n",
    "test_df, test_gs = load_and_concat(test_path)\n",
    "\n",
    "train_df.shape, train_gs.shape,test_df.shape, test_gs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence0</th>\n",
       "      <th>sentence1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But other sources close to the sale said Viven...</td>\n",
       "      <td>But other sources close to the sale said Viven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Micron has declared its first quarterly profit...</td>\n",
       "      <td>Micron's numbers also marked the first quarter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The fines are part of failed Republican effort...</td>\n",
       "      <td>Perry said he backs the Senate's efforts, incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The American Anglican Council, which represent...</td>\n",
       "      <td>The American Anglican Council, which represent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The tech-loaded Nasdaq composite rose 20.96 po...</td>\n",
       "      <td>The technology-laced Nasdaq Composite Index &lt;....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence0  \\\n",
       "0  But other sources close to the sale said Viven...   \n",
       "1  Micron has declared its first quarterly profit...   \n",
       "2  The fines are part of failed Republican effort...   \n",
       "3  The American Anglican Council, which represent...   \n",
       "4  The tech-loaded Nasdaq composite rose 20.96 po...   \n",
       "\n",
       "                                           sentence1  \n",
       "0  But other sources close to the sale said Viven...  \n",
       "1  Micron's numbers also marked the first quarter...  \n",
       "2  Perry said he backs the Senate's efforts, incl...  \n",
       "3  The American Anglican Council, which represent...  \n",
       "4  The technology-laced Nasdaq Composite Index <....  "
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_dataset(dataset):\n",
    "    for column in dataset.columns:\n",
    "        dataset[column] = dataset[column].apply(auto_spell)\n",
    "    return dataset\n",
    "\n",
    "# corrected_train = correct_dataset(train_df)\n",
    "# corrected_train.to_csv('corrected_train.csv')\n",
    "# corrected_test = correct_dataset(test_df)\n",
    "# corrected_test.to_csv('corrected_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('corrected_train.csv', index_col=0)\n",
    "test_df = pd.read_csv('corrected_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_spell(text):\n",
    "    spell = SpellChecker()\n",
    "    misspelled = spell.unknown(text.split())\n",
    "    tagger = PerceptronTagger()\n",
    "    tagged_text = tagger.tag(text)\n",
    "#     print(misspelled)\n",
    "    corrected_text = ''\n",
    "    for i, word in enumerate(text.split()):\n",
    "        if word in misspelled:\n",
    "            tag = tagged_text[i][1]\n",
    "            if tag != 'NNP':\n",
    "                word = spell.correction(word)\n",
    "        corrected_text +=word +' '\n",
    "    return corrected_text.strip()\n",
    "\n",
    "def sentence_lenght(s):\n",
    "    return len(s.split())\n",
    "\n",
    "def count_symbols(s):\n",
    "    count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "    return count(s,set(string.punctuation))\n",
    "\n",
    "def count_shared_words(s0,s1):\n",
    "    list3 = list(set(lemmatize_text(s0.lower()))&set(lemmatize_text(s1.lower())))\n",
    "    return len(list3)\n",
    "\n",
    "def count_digits(s):\n",
    "    numbers = sum(c.isdigit() for c in s)\n",
    "    return numbers\n",
    "\n",
    "def synonim_words(a,b):\n",
    "    return len(set(_get_word_synonyms(a))&set(_get_word_synonyms(b))) > 0\n",
    "                                                                      \n",
    "def count_synonims(s0,s1):\n",
    "    sinonim = 0\n",
    "    for a in s0.split():\n",
    "        for b in s1.split():\n",
    "            sinonim += synonim_words(a,b)\n",
    "    return sinonim\n",
    "\n",
    "def count_common_propper_nouns(s0,s1):\n",
    "    tagger = PerceptronTagger()\n",
    "    s0_tags = tagger.tag(s0.split())\n",
    "    s1_tags = tagger.tag(s1.split())\n",
    "    NNP_s0 = [values[0] for values in s0_tags if values[1] =='NNP']\n",
    "    NNP_s1 = [values[0] for values in s1_tags if values[1] =='NNP']\n",
    "    return len(set(NNP_s0)&set(NNP_s1))\n",
    "\n",
    "def count_nouns(s0):\n",
    "    tagger = PerceptronTagger()\n",
    "    s0_tags = tagger.tag(s0.split())\n",
    "    NN_s0 = [values[0] for values in s0_tags if values[1] =='NN']\n",
    "    return len(NN_s0)\n",
    "\n",
    "def count_verbs(s0):\n",
    "    tagger = PerceptronTagger()\n",
    "    s0_tags = tagger.tag(s0.split())\n",
    "    V_s0 = [values[0] for values in s0_tags if values[1] =='VBP']\n",
    "    return len(V_s0)\n",
    "\n",
    "def remove_stop_words(s0):\n",
    "    new_vector = ' '.join(word for word in s0.split() if word.lower() not in list(stopwords.words('english')))\n",
    "    return new_vector\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tagger = PerceptronTagger()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    w_tokenizer = WhitespaceTokenizer()\n",
    "    s_tokenized = w_tokenizer.tokenize(text)\n",
    "    s_tagged = tagger.tag(s_tokenized)\n",
    "    return [lemmatizer.lemmatize(w[0],penn_to_wn(w[1])) for w in s_tagged]\n",
    "\n",
    "def calculate_jaccard(s0,s1):\n",
    "    lemms_0 = [a for  a in lemmatize_text(s0) if a]\n",
    "    lemms_1 = [a for  a in lemmatize_text(s1) if a]\n",
    "    \n",
    "    jaccard_simmilarity = (1 - jaccard_distance(set(lemms_0), set(lemms_1)))\n",
    "    return jaccard_simmilarity\n",
    "\n",
    "def _get_word_synonyms(word):\n",
    "    word_synonyms = []\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for lemma in synset.lemma_names():\n",
    "            word_synonyms.append(lemma)\n",
    "    return word_synonyms\n",
    "\n",
    "def synonim_proportion(s0,s1):\n",
    "    syn_count = 0\n",
    "    for a in s0.split():\n",
    "        synonims_a = _get_word_synonyms(a)\n",
    "        for b in s1.split():\n",
    "            synonims_b = _get_word_synonyms(b)\n",
    "            if a ==b:\n",
    "                are_syns = 1\n",
    "            else:\n",
    "                are_syns = len(set(_get_word_synonyms(a))&set(_get_word_synonyms(b))) > 0\n",
    "#             print(a,b,are_syns)\n",
    "            syn_count += are_syns\n",
    "    max_len = min([len(s0.split()),len(s1.split())])\n",
    "#     print(syn_count, max_len)\n",
    "    return syn_count/max_len\n",
    "\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    "    return 'n'\n",
    " \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def sentence_similarity(sentence1, sentence2,similarity=wn.path_similarity):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(lemmatize_text(sentence1))\n",
    "    sentence2 = pos_tag(lemmatize_text(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    "\n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        similarities = [similarity(synset,ss) for ss in synsets2 if similarity(synset,ss) ]\n",
    "        try:\n",
    "            best_score = max(similarities)\n",
    "        except:\n",
    "            best_score = 0\n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score += best_score\n",
    "            count += 1\n",
    "    # Average the values\n",
    "    try:\n",
    "        score /= count\n",
    "    except:\n",
    "        score = 0\n",
    "    return score\n",
    "\n",
    "\n",
    "def sentence_similarity_information_content(sentence1, sentence2,similarity):\n",
    "    ''' compute the sentence similairty using information content from wordnet '''\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(lemmatize_text(sentence1))\n",
    "    sentence2 = pos_tag(lemmatize_text(sentence2))\n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    "    score, count = 0.0, 0\n",
    "    ppdb_score, align_cnt = 0, 0\n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        L = []\n",
    "        for ss in synsets2:\n",
    "            try:\n",
    "                L.append(wn.similarity(synset,ss, brown_ic))\n",
    "            except:\n",
    "                continue\n",
    "        if L: \n",
    "            best_score = max(L)\n",
    "            score += best_score\n",
    "            count += 1\n",
    "    # Average the values\n",
    "    if count >0: score /= count\n",
    "    return score\n",
    "\n",
    "\n",
    "def comon_stop_word_proportion(s0,s1):\n",
    "    stopwords_s0 = [word.lower() for word in s0.split() if word.lower() in list(stopwords.words('english'))]\n",
    "    stopwords_s1 = [word.lower() for word in s1.split() if word.lower() in list(stopwords.words('english'))]\n",
    "    common = len(set(stopwords_s0)&set(stopwords_s1))\n",
    "    if min(len(stopwords_s0),len(stopwords_s1)) > 0:\n",
    "        return common/min(len(stopwords_s0),len(stopwords_s1))\n",
    "    return 0\n",
    "    \n",
    "\n",
    "def feature_extractor(dataset):\n",
    "    features = pd.DataFrame(columns=['sentence_0_lengh','sentence_1_lengh',\n",
    "                                    'number_of_nouns_s0', 'number_of_nouns_s1',\n",
    "                                    'number_of_verbs_s0', 'number_of_verbs_s1',\n",
    "                                    'number_of_symbols_s0','number_of_symbols_s1',\n",
    "                                   'number_of_digits_s0','number_of_digits_1',\n",
    "                                   'synonim_proportion','quantity_of_shared_words', \n",
    "                                    'proper_nouns_shared','jaccard_distance','path_similarity',\n",
    "                                    'wup_similarity','comon_stop_word_proportion','resnik_similarity', \n",
    "                                     'jcn_similarity','lin_similarity'])\n",
    "    \n",
    "    for index, row in dataset.iterrows():\n",
    "        s0 = row['sentence0']\n",
    "        s1 = row['sentence1']\n",
    "        features.loc[index,'comon_stop_word_proportion'] = comon_stop_word_proportion(s0,s1)\n",
    "\n",
    "    for column in dataset.columns:\n",
    "        dataset[column] = dataset[column].apply(remove_stop_words)\n",
    "        dataset[column] = dataset[column].str.replace('\\d+', '')\n",
    "    \n",
    "    for index, row in dataset.iterrows():\n",
    "        s0 = row['sentence0']\n",
    "        s1 = row['sentence1']\n",
    "        features.loc[index,'jaccard_distance'] = calculate_jaccard(s0,s1)\n",
    "        features.loc[index,'resnik_similarity'] = sentence_similarity_information_content(s0,s1,wn.res_similarity)\n",
    "        features.loc[index,'jcn_similarity'] = sentence_similarity_information_content(s0,s1,wn.jcn_similarity)\n",
    "        features.loc[index,'lin_similarity'] = sentence_similarity_information_content(s0,s1,wn.lin_similarity)\n",
    "        features.loc[index,'path_similarity'] = sentence_similarity(s0,s1,wn.path_similarity)\n",
    "        features.loc[index,'wup_similarity'] = sentence_similarity(s0,s1,wn.wup_similarity)\n",
    "        features.loc[index,'proper_nouns_shared'] = count_common_propper_nouns(s0,s1)\n",
    "        features.loc[index,'quantity_of_shared_words'] = count_shared_words(s0,s1)\n",
    "        features.loc[index,'synonim_proportion'] = synonim_proportion(s0,s1)\n",
    "        features.loc[index,'sentence_0_lengh'] = sentence_lenght(s0)\n",
    "        features.loc[index,'sentence_1_lengh'] = sentence_lenght(s1)\n",
    "        features.loc[index,'number_of_nouns_s0'] = count_nouns(s0)\n",
    "        features.loc[index,'number_of_nouns_s1'] = count_nouns(s1)\n",
    "        features.loc[index,'number_of_verbs_s0'] = count_verbs(s0)\n",
    "        features.loc[index,'number_of_verbs_s1'] = count_verbs(s1)\n",
    "        features.loc[index,'number_of_symbols_s0'] = count_symbols(s0)\n",
    "        features.loc[index,'number_of_symbols_s1'] = count_symbols(s1)\n",
    "        features.loc[index,'number_of_digits_s0'] = count_digits(s0)\n",
    "        features.loc[index,'number_of_digits_1'] = count_digits(s1)\n",
    "    features['resnik_similarity'] = scaler.fit_transform(features[['resnik_similarity']].values)\n",
    "    return features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = feature_extractor(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_0_lengh</th>\n",
       "      <th>sentence_1_lengh</th>\n",
       "      <th>number_of_nouns_s0</th>\n",
       "      <th>number_of_nouns_s1</th>\n",
       "      <th>number_of_verbs_s0</th>\n",
       "      <th>number_of_verbs_s1</th>\n",
       "      <th>number_of_symbols_s0</th>\n",
       "      <th>number_of_symbols_s1</th>\n",
       "      <th>number_of_digits_s0</th>\n",
       "      <th>number_of_digits_1</th>\n",
       "      <th>synonim_proportion</th>\n",
       "      <th>quantity_of_shared_words</th>\n",
       "      <th>proper_nouns_shared</th>\n",
       "      <th>jaccard_distance</th>\n",
       "      <th>path_similarity</th>\n",
       "      <th>wup_similarity</th>\n",
       "      <th>comon_stop_word_proportion</th>\n",
       "      <th>resnik_similarity</th>\n",
       "      <th>jcn_similarity</th>\n",
       "      <th>lin_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.555839</td>\n",
       "      <td>0.659184</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.636905</td>\n",
       "      <td>0.694805</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.16667</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.37047</td>\n",
       "      <td>0.564367</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_0_lengh sentence_1_lengh number_of_nouns_s0 number_of_nouns_s1  \\\n",
       "0               16               12                  8                  5   \n",
       "1                7               11                  4                  6   \n",
       "2                9               10                  5                  4   \n",
       "3               12               17                  4                  8   \n",
       "4               10                9                  7                  7   \n",
       "\n",
       "  number_of_verbs_s0 number_of_verbs_s1 number_of_symbols_s0  \\\n",
       "0                  2                  1                    0   \n",
       "1                  0                  0                    0   \n",
       "2                  1                  2                    0   \n",
       "3                  1                  1                    0   \n",
       "4                  0                  0                    0   \n",
       "\n",
       "  number_of_symbols_s1 number_of_digits_s0 number_of_digits_1  \\\n",
       "0                    0                   0                  0   \n",
       "1                    0                   0                  0   \n",
       "2                    0                   0                  0   \n",
       "3                    0                   0                  0   \n",
       "4                    0                   0                  0   \n",
       "\n",
       "  synonim_proportion quantity_of_shared_words proper_nouns_shared  \\\n",
       "0               0.75                        9                   0   \n",
       "1           0.857143                        6                   0   \n",
       "2           0.777778                        5                   0   \n",
       "3            1.16667                       11                   0   \n",
       "4           0.777778                        3                   0   \n",
       "\n",
       "  jaccard_distance path_similarity wup_similarity comon_stop_word_proportion  \\\n",
       "0         0.473684        0.555839       0.659184                          0   \n",
       "1              0.5               1              1                          0   \n",
       "2         0.357143        0.636905       0.694805                          0   \n",
       "3         0.611111        0.907407       0.949495                          0   \n",
       "4           0.1875         0.37047       0.564367                          0   \n",
       "\n",
       "   resnik_similarity jcn_similarity lin_similarity  \n",
       "0                0.0              0              0  \n",
       "1                0.0              0              0  \n",
       "2                0.0              0              0  \n",
       "3                0.0              0              0  \n",
       "4                0.0              0              0  "
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = feature_extractor(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3108, 20)"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarize features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "train_features_std = scaler.fit_transform(train_features)\n",
    "test_features_std = scaler.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    tagger = PerceptronTagger()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    w_tokenizer = WhitespaceTokenizer()\n",
    "    s_tokenized = w_tokenizer.tokenize(text)\n",
    "    s_tagged = tagger.tag(s_tokenized)\n",
    "    return [lemmatizer.lemmatize(w[0],penn_to_wn(w[1])) for w in s_tagged]\n",
    "\n",
    "\n",
    "def preprocessing(data, return_array = False):\n",
    "    # todo: better handling of na\n",
    "    data = data.fillna('')\n",
    "    for column in data.columns:\n",
    "        print(column)\n",
    "        # remove the digits and puntuation\n",
    "#         data[column] = data[column].str.replace('\\d+', '')\n",
    "        # convert to lowercase\n",
    "        data[column] = data[column].str.replace('\\W+', ' ')\n",
    "        # replace continuous white spaces by a single one\n",
    "        data[column] = data[column].str.replace('\\s+', ' ')\n",
    "        # words to lower\n",
    "        data[column] = data[column].str.lower()\n",
    "        # lematize\n",
    "        data[column] = data[column].apply(lemmatize_text)\n",
    "        if not return_array:\n",
    "            data[column] = data[column].str.join(' ')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence0\n",
      "sentence1\n",
      "sentence0\n",
      "sentence1\n"
     ]
    }
   ],
   "source": [
    "train_df = preprocessing(train_df)\n",
    "test_df = preprocessing(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence0</th>\n",
       "      <th>sentence1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>source close sale say vivendi keep door open b...</td>\n",
       "      <td>source close sale say vivendi keep door open b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>micron declare first quarterly profit three year</td>\n",
       "      <td>micron number also mark first quarterly profit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fine part fail republican effort force entice ...</td>\n",
       "      <td>perry say back senate effort include fine forc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>american anglican council represent episcopali...</td>\n",
       "      <td>american anglican council represent episcopali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tech load nasdaq composite rise point end high...</td>\n",
       "      <td>technology lace nasdaq composite index ixic cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence0  \\\n",
       "0  source close sale say vivendi keep door open b...   \n",
       "1   micron declare first quarterly profit three year   \n",
       "2  fine part fail republican effort force entice ...   \n",
       "3  american anglican council represent episcopali...   \n",
       "4  tech load nasdaq composite rise point end high...   \n",
       "\n",
       "                                           sentence1  \n",
       "0  source close sale say vivendi keep door open b...  \n",
       "1  micron number also mark first quarterly profit...  \n",
       "2  perry say back senate effort include fine forc...  \n",
       "3  american anglican council represent episcopali...  \n",
       "4  technology lace nasdaq composite index ixic cl...  "
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence0</th>\n",
       "      <th>sentence1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>problem likely mean corrective change shuttle ...</td>\n",
       "      <td>say problem need correct space shuttle fleet c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>technology lace nasdaq composite index ixic in...</td>\n",
       "      <td>broad standard door index spx inch point percent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>let huge black eye say publisher arthur chs su...</td>\n",
       "      <td>let huge black eye arthur sulzberger newspaper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sec chairman william donaldson say build confi...</td>\n",
       "      <td>think three build confidence cop beat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vivendi share close percent euro paris fall pe...</td>\n",
       "      <td>new work vivendi share percent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence0  \\\n",
       "0  problem likely mean corrective change shuttle ...   \n",
       "1  technology lace nasdaq composite index ixic in...   \n",
       "2  let huge black eye say publisher arthur chs su...   \n",
       "3  sec chairman william donaldson say build confi...   \n",
       "4  vivendi share close percent euro paris fall pe...   \n",
       "\n",
       "                                           sentence1  \n",
       "0  say problem need correct space shuttle fleet c...  \n",
       "1   broad standard door index spx inch point percent  \n",
       "2  let huge black eye arthur sulzberger newspaper...  \n",
       "3              think three build confidence cop beat  \n",
       "4                     new work vivendi share percent  "
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pearson:  0.7240944415659323\n",
      "test pearson:  0.5759821366396134\n"
     ]
    }
   ],
   "source": [
    "def lexical_simmilarity(df):\n",
    "    guess = pd.DataFrame()\n",
    "    for i in df.index:\n",
    "        guess.loc[i,'labels'] = 1 - jaccard_distance(set(df.loc[i,'sentence0']), set(df.loc[i,'sentence1']))\n",
    "    return guess\n",
    "\n",
    "\n",
    "\n",
    "guess_lex_train = lexical_simmilarity(train_df)\n",
    "guess_lex_test = lexical_simmilarity(test_df)\n",
    "\n",
    "print('train pearson: ', pearsonr(guess_lex_train['labels'], train_gs['labels'])[0])\n",
    "print('test pearson: ', pearsonr(guess_lex_test['labels'], test_gs['labels'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pearson:  0.525726893303768\n",
      "test pearson: 0.591786275851825\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "\n",
    "tfv = TfidfVectorizer(max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(train_df['sentence1']) + list(train_df['sentence0']) )\n",
    "\n",
    "def return_simil(a,b):\n",
    "    simil = tfv.transform([a,b])\n",
    "    return ((simil * simil.T).A)[0,1]\n",
    "\n",
    "def calculate_all_sims(df):\n",
    "    results = []\n",
    "    for i in df.values:\n",
    "        results.append(return_simil(i[0], i[1]))\n",
    "    return results\n",
    "\n",
    "\n",
    "all_sims = calculate_all_sims(train_df)\n",
    "test_sims = calculate_all_sims(test_df)\n",
    "\n",
    "print('train pearson: ', pearsonr(all_sims, train_gs['labels'])[0])\n",
    "print('test pearson:', pearsonr(test_sims, test_gs['labels'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merged train with TfidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sentences = train_df['sentence0'] + train_df['sentence1']\n",
    "merged_test = test_df['sentence0'] + test_df['sentence1']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "merged_train = vectorizer.fit_transform(merged_sentences)\n",
    "merged_test = vectorizer.transform(merged_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence0\n",
      "sentence1\n",
      "No of words in the dictionary = 1705\n",
      "sentence0\n",
      "sentence1\n",
      "(2234, 1705)\n",
      "(3108, 1705)\n"
     ]
    }
   ],
   "source": [
    "def train_dictionary(df):\n",
    "    \n",
    "    sentences_tokenized = df.sentence0.tolist() + df.sentence1.tolist()\n",
    "    \n",
    "    dictionary = corpora.Dictionary(sentences_tokenized)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.8)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    return dictionary\n",
    "    \n",
    "def get_vectors(df, dictionary):\n",
    "    \n",
    "    sentence0_vec = [dictionary.doc2bow(text) for text in df.sentence0.tolist()]\n",
    "    sentence1_vec = [dictionary.doc2bow(text) for text in df.sentence1.tolist()]\n",
    "    \n",
    "    sentence0_csc = gensim.matutils.corpus2csc(sentence0_vec, num_terms=len(dictionary.token2id))\n",
    "    sentence1_csc = gensim.matutils.corpus2csc(sentence1_vec, num_terms=len(dictionary.token2id))\n",
    "    \n",
    "    return sentence0_csc.transpose(),sentence1_csc.transpose()\n",
    "\n",
    "tokenized_train = preprocessing(train_df, return_array = True)\n",
    "dictionary = train_dictionary(tokenized_train)\n",
    "print (\"No of words in the dictionary = %s\" %len(dictionary))\n",
    "\n",
    "tokenized_test = preprocessing(test_df, return_array = True)\n",
    "\n",
    "q1_csc, q2_csc = get_vectors(tokenized_train, dictionary)\n",
    "q1_csc_test, q2_csc_test = get_vectors(tokenized_test, dictionary)\n",
    "\n",
    "print (q1_csc.shape)\n",
    "print (q1_csc_test.shape)\n",
    "\n",
    "train_bog = np.concatenate((q1_csc.todense(), q2_csc.todense()), axis=1)\n",
    "test_bog = np.concatenate((q1_csc_test.todense(), q2_csc_test.todense()), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bog_extended = pd.concat([pd.DataFrame(train_bog),train_features],axis=1)\n",
    "test_bog_extended = pd.concat([pd.DataFrame(test_bog),test_features],axis=1)\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_minmax = scaler.fit_transform(train_bog_extended)\n",
    "X_test_minmax = scaler.fit_transform(test_bog_extended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,xtrain,xtest):\n",
    "    train_predicted =  model.predict(xtrain)\n",
    "    test_predicted =   model.predict(xtest)\n",
    "    print('train pearson: ', pearsonr(train_predicted, train_gs['labels'])[0])\n",
    "    print('test pearson: ', pearsonr(test_predicted, test_gs['labels'])[0])\n",
    "\n",
    "def train_and_test_model(model, train,test,model_name='model'):\n",
    "    model.fit(train,train_gs)\n",
    "    test_model(model,train,test)\n",
    "    if model_name == 'rfr':\n",
    "        print_feature_importance(rfr,train)\n",
    "\n",
    "    \n",
    "\n",
    "def print_feature_importance(rfr,train):\n",
    "    importances=rfr.feature_importances_ ## get the feature importance\n",
    "    # print(\"Original \",np.argsort(importances))\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    try:\n",
    "        feat_labels = train.columns\n",
    "    except:\n",
    "        return\n",
    "    for f in range(10):\n",
    "        print(\"%2d) %-*s %f\" % (f+1,30,feat_labels[indices[f]],\n",
    "                                        importances[indices[f]]))\n",
    "        \n",
    "def run_with_all_datasets(model,model_name):\n",
    "    print(model_name)\n",
    "    print('Only Features')\n",
    "    train_and_test_model(model,train_features,test_features,model_name)\n",
    "    \n",
    "    print('Only Features standarized')\n",
    "    train_and_test_model(model,train_features_std,test_features_std,model_name)\n",
    "\n",
    "    print('Only TifVectorizer')\n",
    "    train_and_test_model(model,merged_train,merged_test,model_name)\n",
    "    print('Only Bag of Words')\n",
    "    train_and_test_model(model,train_bog,test_bog,model_name)\n",
    "    print('Bag of Words + features')\n",
    "    train_and_test_model(model,train_bog_extended,test_bog_extended,model_name)\n",
    "    print('Bag of words standarized')\n",
    "    train_and_test_model(model,X_train_minmax,X_test_minmax,model_name)\n",
    "\n",
    "\n",
    "def show_worst_test(predicted, k=5):\n",
    "    print('Worst results in voting:')\n",
    "    err = np.abs(predicted - test_gs['labels'])\n",
    "    idx = np.argpartition(err, -k)[-k:]\n",
    "    for i in idx:\n",
    "        print(test_df.loc[i,'sentence0'],'\\n',test_df.loc[i,'sentence1'] ,'\\noriginal',test_gs.loc[i,'labels'],'predicted', predicted[i])\n",
    "        print(original_test.loc[i,'sentence0'],'\\n',original_test.loc[i,'sentence1'] )\n",
    "        print(test_features.loc[i,['jaccard_distance','path_similarity',\n",
    "                                    'wup_similarity']])\n",
    "        print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks\n",
      "Only Features\n",
      "train pearson:  0.7912211001317124\n",
      "test pearson:  0.671466363081375\n",
      "Only Features standarized\n",
      "train pearson:  0.8239533352548011\n",
      "test pearson:  0.6658800561696351\n",
      "Only TifVectorizer\n",
      "train pearson:  0.9832960571332431\n",
      "test pearson:  0.4647900179679494\n",
      "Only Bag of Words\n",
      "train pearson:  0.9810854398499554\n",
      "test pearson:  0.22920719395847114\n",
      "Bag of Words + features\n",
      "train pearson:  0.9842520271264595\n",
      "test pearson:  0.6572840792351324\n",
      "Bag of words standarized\n",
      "train pearson:  0.9781280057104583\n",
      "test pearson:  0.6495910843574755\n",
      "Bag of words standarized\n",
      "train pearson:  0.9859363749904141\n",
      "test pearson:  0.37813949189770013\n"
     ]
    }
   ],
   "source": [
    "model_nn = MLPRegressor(hidden_layer_sizes=(100,100),validation_fraction=0.3, alpha=0.3,warm_start=False,max_iter=1000)\n",
    "run_with_all_datasets(model_nn,'Neural networks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {'alpha': 10.0 ** -np.arange(0, 10), 'max_iter':[2000],\n",
    "              'hidden_layer_sizes':np.arange(1, 8),'solver': ['lbfgs','adam'],'warm_start': [False]}\n",
    "nn_cv = GridSearchCV(MLPRegressor(), parameters, n_jobs=-1)\n",
    "# train_and_test_model(nn_cv, train_features_std,test_features_std,model_name='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_0_lengh</th>\n",
       "      <th>sentence_1_lengh</th>\n",
       "      <th>number_of_nouns_s0</th>\n",
       "      <th>number_of_nouns_s1</th>\n",
       "      <th>number_of_verbs_s0</th>\n",
       "      <th>number_of_verbs_s1</th>\n",
       "      <th>number_of_symbols_s0</th>\n",
       "      <th>number_of_symbols_s1</th>\n",
       "      <th>number_of_digits_s0</th>\n",
       "      <th>number_of_digits_1</th>\n",
       "      <th>synonim_proportion</th>\n",
       "      <th>quantity_of_shared_words</th>\n",
       "      <th>proper_nouns_shared</th>\n",
       "      <th>jaccard_distance</th>\n",
       "      <th>path_similarity</th>\n",
       "      <th>wup_similarity</th>\n",
       "      <th>comon_stop_word_proportion</th>\n",
       "      <th>resnik_similarity</th>\n",
       "      <th>jcn_similarity</th>\n",
       "      <th>lin_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.555839</td>\n",
       "      <td>0.659184</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.636905</td>\n",
       "      <td>0.694805</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.16667</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.37047</td>\n",
       "      <td>0.564367</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_0_lengh sentence_1_lengh number_of_nouns_s0 number_of_nouns_s1  \\\n",
       "0               16               12                  8                  5   \n",
       "1                7               11                  4                  6   \n",
       "2                9               10                  5                  4   \n",
       "3               12               17                  4                  8   \n",
       "4               10                9                  7                  7   \n",
       "\n",
       "  number_of_verbs_s0 number_of_verbs_s1 number_of_symbols_s0  \\\n",
       "0                  2                  1                    0   \n",
       "1                  0                  0                    0   \n",
       "2                  1                  2                    0   \n",
       "3                  1                  1                    0   \n",
       "4                  0                  0                    0   \n",
       "\n",
       "  number_of_symbols_s1 number_of_digits_s0 number_of_digits_1  \\\n",
       "0                    0                   0                  0   \n",
       "1                    0                   0                  0   \n",
       "2                    0                   0                  0   \n",
       "3                    0                   0                  0   \n",
       "4                    0                   0                  0   \n",
       "\n",
       "  synonim_proportion quantity_of_shared_words proper_nouns_shared  \\\n",
       "0               0.75                        9                   0   \n",
       "1           0.857143                        6                   0   \n",
       "2           0.777778                        5                   0   \n",
       "3            1.16667                       11                   0   \n",
       "4           0.777778                        3                   0   \n",
       "\n",
       "  jaccard_distance path_similarity wup_similarity comon_stop_word_proportion  \\\n",
       "0         0.473684        0.555839       0.659184                          0   \n",
       "1              0.5               1              1                          0   \n",
       "2         0.357143        0.636905       0.694805                          0   \n",
       "3         0.611111        0.907407       0.949495                          0   \n",
       "4           0.1875         0.37047       0.564367                          0   \n",
       "\n",
       "   resnik_similarity jcn_similarity lin_similarity  \n",
       "0                0.0              0              0  \n",
       "1                0.0              0              0  \n",
       "2                0.0              0              0  \n",
       "3                0.0              0              0  \n",
       "4                0.0              0              0  "
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfr\n",
      "Only Features\n",
      "train pearson:  0.9779543599550378\n",
      "test pearson:  0.6555399617134543\n",
      " 1) quantity_of_shared_words       0.513356\n",
      " 2) synonim_proportion             0.098693\n",
      " 3) path_similarity                0.089151\n",
      " 4) wup_similarity                 0.075721\n",
      " 5) jaccard_distance               0.073515\n",
      " 6) sentence_1_lengh               0.037266\n",
      " 7) sentence_0_lengh               0.035217\n",
      " 8) number_of_nouns_s0             0.030619\n",
      " 9) number_of_nouns_s1             0.025988\n",
      "10) number_of_verbs_s0             0.011970\n",
      "Only Bag of Words\n",
      "Bag of Words + features\n",
      "train pearson:  0.9818445786318116\n",
      "test pearson:  0.7414825359889686\n",
      " 1) quantity_of_shared_words       0.501298\n",
      " 2) synonim_proportion             0.070279\n",
      " 3) jaccard_distance               0.046341\n",
      " 4) path_similarity                0.030808\n",
      " 5) 43                             0.027008\n",
      " 6) wup_similarity                 0.021099\n",
      " 7) sentence_1_lengh               0.020730\n",
      " 8) sentence_0_lengh               0.014888\n",
      " 9) 86                             0.009482\n",
      "10) number_of_nouns_s0             0.008730\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(n_jobs=-1,n_estimators=1000)\n",
    "run_with_all_datasets(rfr,'rfr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test pearson:  0.7237711453046553\n"
     ]
    }
   ],
   "source": [
    "rfr.fit(X_train_minmax, train_gs['labels'])\n",
    "print_feature_importance(rfr,X_train_minmax)\n",
    "\n",
    "test_predicted = rfr.predict(X_test_minmax)\n",
    "print('test pearson: ', pearsonr(test_predicted, test_gs['labels'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst results in voting:\n",
      "way need \n",
      " mean find \n",
      "original 4.6 predicted 1.1704659999999998\n",
      "Other ways are needed. \n",
      " Other means should be found.\n",
      "jaccard_distance            0\n",
      "path_similarity     0.0801282\n",
      "wup_similarity       0.258333\n",
      "Name: 2058, dtype: object\n",
      "-------------------------------------------\n",
      "necessary \n",
      " need \n",
      "original 5.0 predicted 0.970081666666668\n",
      "But they were necessary. \n",
      " But they were needed.  \n",
      "jaccard_distance    0\n",
      "path_similarity     0\n",
      "wup_similarity      0\n",
      "Name: 2287, dtype: object\n",
      "-------------------------------------------\n",
      "sex \n",
      " sexual intercourse \n",
      "original 5.0 predicted 1.057834666666668\n",
      " have sex with \n",
      " have sexual intercourse with.\n",
      "jaccard_distance            0\n",
      "path_similarity     0.0769231\n",
      "wup_similarity       0.142857\n",
      "Name: 2664, dtype: object\n",
      "-------------------------------------------\n",
      "resist \n",
      " act opposition \n",
      "original 4.5 predicted 0.9384826666666679\n",
      "be against, resist \n",
      " act against or in opposition to.\n",
      "jaccard_distance    0\n",
      "path_similarity     0\n",
      "wup_similarity      0\n",
      "Name: 2586, dtype: object\n",
      "-------------------------------------------\n",
      "way need \n",
      " mean find \n",
      "original 4.6 predicted 1.1704659999999998\n",
      "Other ways are needed. \n",
      " Other means should be found.\n",
      "jaccard_distance            0\n",
      "path_similarity     0.0801282\n",
      "wup_similarity       0.258333\n",
      "Name: 2275, dtype: object\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "show_worst_test(test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test pearson:  0.7005426445724035\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=140)\n",
    "gbr.fit(X_train_minmax,train_gs['labels'])\n",
    "test_predicted = gbr.predict(X_test_minmax)\n",
    "print('test pearson: ', pearsonr(test_predicted, test_gs['labels'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pearson:  0.9138642102536191\n",
      "test pearson:  0.5354240328571851\n"
     ]
    }
   ],
   "source": [
    "svr = SVR()\n",
    "train_and_test_model(svr, X_train_minmax,X_test_minmax,model_name='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try using all the distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "minkowski_dis = DistanceMetric.get_metric('minkowski')\n",
    "mms_scale_man = MinMaxScaler()\n",
    "mms_scale_euc = MinMaxScaler()\n",
    "mms_scale_mink = MinMaxScaler()\n",
    "\n",
    "def get_similarity_values(q1_csc, q2_csc):\n",
    "    cosine_sim = []\n",
    "    manhattan_dis = []\n",
    "    eucledian_dis = []\n",
    "    jaccard_dis = []\n",
    "    minkowsk_dis = []\n",
    "    \n",
    "    for i,j in zip(q1_csc, q2_csc):\n",
    "        sim = cs(i,j)\n",
    "        cosine_sim.append(sim[0][0])\n",
    "        sim = md(i,j)\n",
    "        manhattan_dis.append(sim[0][0])\n",
    "        sim = ed(i,j)\n",
    "        eucledian_dis.append(sim[0][0])\n",
    "        i_ = i.toarray()\n",
    "        j_ = j.toarray()\n",
    "        try:\n",
    "            sim = jsc(i_,j_)\n",
    "            jaccard_dis.append(sim)\n",
    "        except:\n",
    "            jaccard_dis.append(0)\n",
    "            \n",
    "        sim = minkowski_dis.pairwise(i_,j_)\n",
    "        minkowsk_dis.append(sim[0][0])\n",
    "    \n",
    "    return cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis    \n",
    "\n",
    "\n",
    "# cosine_sim = get_cosine_similarity(q1_csc, q2_csc)\n",
    "cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis = get_similarity_values(q1_csc, q2_csc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test pearson:  0.44169660385086723\n",
      "test pearson:  -0.334519037932084\n",
      "test pearson:  -0.36185335508446154\n",
      "test pearson:  0.42917921822900995\n",
      "test pearson:  -0.36185335508446154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def calculate_logloss(y_true, y_pred):\n",
    "    loss_cal = log_loss(y_true, y_pred)\n",
    "    return loss_cal\n",
    "\n",
    "y_pred_cos, y_pred_man, y_pred_euc, y_pred_jac, y_pred_mink = get_similarity_values(q1_csc_test, q2_csc_test)\n",
    "predictions = [y_pred_cos, y_pred_man, y_pred_euc, y_pred_jac, y_pred_mink]\n",
    "for test_predicted in predictions:\n",
    "    print('test pearson: ', pearsonr(test_predicted, test_gs['labels'])[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test pearson:  0.45387028522267225\n",
      "test pearson:  0.4969014705417928\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "X_train = pd.DataFrame({\"cos\" : cosine_sim, \"man\" : manhattan_dis, \"euc\" : eucledian_dis, \"jac\" : jaccard_dis, \"min\" : minkowsk_dis})\n",
    "\n",
    "X_test = pd.DataFrame({\"cos\" : y_pred_cos, \"man\" : y_pred_man, \"euc\" : y_pred_euc, \"jac\" : y_pred_jac, \"min\" : y_pred_mink})\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(X_train,train_gs.values.ravel())\n",
    "\n",
    "svr = SVR()\n",
    "svr.fit(X_train,train_gs.values.ravel())\n",
    "\n",
    "y_rfr_predicted = rfr.predict(X_test)\n",
    "y_svr_predicted = svr.predict(X_test)\n",
    "\n",
    "print('test pearson: ', pearsonr(y_rfr_predicted, test_gs['labels'])[0])\n",
    "print('test pearson: ', pearsonr(y_svr_predicted, test_gs['labels'])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
